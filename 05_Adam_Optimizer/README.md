## [Adam: A Method for Stochastic Optimization of the First Human](https://www.biblegateway.com/passage/?search=Genesis+2%3A4-3%3A24&version=NIV) ðŸ˜‡
## [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
## [On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ)   

### Additional Links:

  * [Slides about Adam](https://moodle2.cs.huji.ac.il/nu15/pluginfile.php/316969/mod_resource/content/1/adam_pres.pdf)
  * [Blogpost explaining SGD Improvements](http://ruder.io/optimizing-gradient-descent/index.html) (including Adam, RMSProp etc.) in layman's terms + [slides](https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning)
  * [Blogpost on Optimization in Deep Learning](http://ruder.io/deep-learning-optimization-2017/index.html)

### Further Discussion:

  * [Discussion of the AMSGrad paper](https://openreview.net/forum?id=ryQu7f-RZ) on OpenReview
  * [Discussion of the AMSGrad paper](https://www.reddit.com/r/MachineLearning/comments/7lfo0w/r_on_the_convergence_of_adam_and_beyond/) on Reddit
  * [Experiments with AMSGrad](https://fdlm.github.io/post/amsgrad/)
  * [Really good paper on Generalization](https://arxiv.org/abs/1705.08292)
  * [Fixing Weight Decay Regularization in Adam](https://openreview.net/pdf?id=rk6qdGgCZ&noteId=H1zlFEcxM)
  * [YellowFin and the Art of Momentum Tuning](https://openreview.net/pdf?id=SyrGJYlRZ)


